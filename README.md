Project Descprition	

This web crawler was developed using a framework called Scrapy, specific for web crawling and structured data extraction.
Basically, it defines the domains that will be covered, the initial requisitions and how to handle the result of these requisitions.
After this, various forms of data output can be generated, including a .CSV file.



STEPS TO RUN THE PROJECT (if you already have scrapy installed)
	
	1. Open the project directory in the terminal
	2. Type 'scrapy crawl agency -o agencies.csv'

Obs: If you don't have Scrapy, just install and follow the process.
